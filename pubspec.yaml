name: dartllm
description: >-
  Run local LLM inference on-device using GGUF models via llama.cpp.
  Supports chat, completions, embeddings, and streaming across all platforms.
version: 0.1.0
homepage: https://github.com/samuelkchris/dartllm
repository: https://github.com/samuelkchris/dartllm
issue_tracker: https://github.com/samuelkchris/dartllm/issues
topics:
  - ai
  - llm
  - machine-learning
  - llama
  - inference

environment:
  sdk: ^3.5.0

dependencies:
  crypto: ^3.0.6
  ffi: ^2.1.3
  freezed_annotation: ^3.1.0
  http: ^1.2.0
  json_annotation: ^4.9.0
  logging: ^1.3.0
  meta: ^1.16.0
  path: ^1.9.0

dev_dependencies:
  build_runner: ^2.4.13
  ffigen: ^20.1.1
  freezed: ^3.2.3
  json_serializable: ^6.8.0
  lints: ^5.1.0
  mockito: ^5.4.4
  test: ^1.25.6
